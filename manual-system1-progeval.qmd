---
title: "Program evaluation - Report template"
author: "Michael Menefee, PhD"
---

These should be created as Quarto HTML documents and published to the OEDA website.

## YAML header

* title: Adopt a title style that clearly indicates the program or intervention, the dependent variable, the size of the observed effect, the location, and the year
    * E.g. "[Program X] reduces [outcome Y] by [meaningful effect size] (2022-2024 statewide data)"
* date: Publication date of final product
* author: Your name and degree
* institution: Oregon Enterprise Data Analytics
* abstract: No more than 500 words, briefly describing the evaluation questions, intervention evaluated, and key findings.

## Executive Summary

The Executive Summary should stand alone as a summary of the key sections of the report and should summarize the content of the full report without adding new information.

* Evaluation Purpose and Evaluation Questions
* Findings, Conclusions, and Recommendations
* Background
* Evaluation Design, Methods, and Limitations

Consider the presentation order above, which is out-of-order relative to the full report below.
Give more focus and space to purpose, questions, findings, conclusions, and recommendations; less to background, design, methods, and limitations.


<!-- ######################################################################## -->

## Evaluation Purpose and Evaluation Questions

This should set out the overarching purpose of the evaluation, and how the findings are expected to be used to inform decisions. This section also describes the evaluation questions (which should be limited to just a few key questions). It can also identify key audiences for the evaluation.

### Evaluation purpose

Clearly describe why the evaluation is being conducted now. Describe the primary and any secondary audiences for the evaluation. Include anticipated uses, including any specific decisions that will be informed by the evaluation.

### Evaluation questions

List the evaluation questions, linking them to the evaluation purpose. Impact evaluations should include questions about measuring the direction and magnitude of change in specific outcomes attributable to what is being evaluated.


<!-- ######################################################################## -->

## Background
Name the relevant agencies, divisions, providers, implementing partners, etc. Describe what is being evaluated: a strategy, intermediate result, project, activity, or intervention.

Describe the problem being addressed by the strategy, project, activity, or interventions and the hypothesis for *why* it will lead to better outcomes. This could include a logical framework for the activity if one exists, and the development hypothesis, or causal logic, of the project or program of which the activity is a part.

Describe the beneficiary population and the geographic area of the intervention.

Consider, too:

* timeline showing dates of implementation of what is being evaluated
* major events impacting implementation of what is being evaluated
* funding levels
* changes that have occurred in the program or context since the intervention started
* deviations in implementation or model fidelity that may confound interpretation of effects


<!-- ######################################################################## -->

## Evaluation Methods and Limitations

This section describes the overall design, specific data collection and analysis methods linked to the evaluation questions, and limitations of the data, methods, or other issues that affected the findings. Describe the methods used and *why* those methods were chosen, including their strengths and limitations. If your methods and limitations approaches three pages or more, consider using an appendix for all the methodological detail, and providing a very concise summary here.

For impact evaluations, describe in detail the method (and data, if applicable) of deriving the comparison (control or quasi-control) group.





Consider:

* Sampling strategy
* Field work (duration, field team composition, field team activities)



<!-- ######################################################################## -->

## Findings, Conclusions, and Recommendations

This section should report those findings based on evidence generated by the evaluation data collection and analysis methods. Findings should be fact-based and not rely only on opinion, even of experts. Conclusions are drawn directly from findings and help summarize the “so what” of the findings. Several findings can lead to one or more conclusions. Whenever possible, data should be presented visually in easy-to-read charts, tables, graphs, and maps to demonstrate the evidence that supports conclusions and recommendations.

### Findings

Findings are empirical facts based on data collected during the evaluation and should not rely only on opinion, even of experts.

### Conclusions

Conclusions synthesize and interpret findings and make judgments supported by one or more specific findings.

The reader should be able to discern what evidence supports the conclusions. Whenever possible, data should be presented visually in easy-to-read charts, tables, graphs, and maps to demonstrate the evidence that supports conclusions. All graphics must have a title, be clearly labeled, and include a caption.

### Recommendations

Recommendations are included if requested in the evaluation statement of work. They are specific actions the evaluation team proposes be taken by program management that are based on findings and conclusions. The reader should be able to discern what evidence supports the conclusions and recommendations.

<!-- ######################################################################## -->


## Appendices


### Timeline of Data Collection

Timeline showing dates of data collection, baseline and subsequent data collection, if applicable.

### Methodological Details

Additional detail not covered in the main body of the report, if needed.

### Data Collection Tools

Any data collection tools employed: questionnaires, checklists, survey instruments, discussion guides, etc.

### Sources of Information

Outside of the material specifically cited in the body of the report, all additional sources of information that informed the evaluation: agency strategy documents, project documents, reporting,  monitoring data, etc.

### 


```{r}
#| echo: true
#| eval: false

---
title: 'Title'
subtitle: 'Optional subtitle'
date: 'yyyy-mm-dd'
author: 'First Last, Degree'
institution: 'Oregon Enterprise Data Analytics'
abstract: 'Abstract text'
---

## Executive Summary

### Evaluation purpose and evaluation questions
  

## Unnumbered heading {.unnumbered}
  
  
```

